{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of bagofwords, ngrams, and Naive Bayes and Logistic Regression\n",
    "\n",
    "- V_4 Data, Target: Google Trend, Row: Hourly articles, Time: 2015 - 2020 w/ gaps, no shift in target time\n",
    "- TF-IDF with ngrams\n",
    "- TF-IDF without ngrams\n",
    "- CountVectorizer with and without ngrams\n",
    "- CountVectorizer with max_features\n",
    "- Naive Bayes\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in gdelt V4 data\n",
    "df = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/master_data_set/text_with_tokens_52k.csv')\n",
    "\n",
    "display(df.shape)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "#extracting date from gkgcode\n",
    "df['date_time']  = df['gkgcode'].apply(lambda x: x[:14])\n",
    "\n",
    "\n",
    "#creating date time object\n",
    "df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "df.set_index('date_time', inplace=True)\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "#resample text data hourly and join text tokens\n",
    "\n",
    "test_resample = df.resample('h')['text'].agg(lambda column: \"\".join(column))\n",
    "\n",
    "display(test_resample.shape)\n",
    "\n",
    "test_resample.apply(lambda x: len(x) < 100).sum()\n",
    "\n",
    "test_resample = test_resample.reset_index()\n",
    "\n",
    "### Reading in Google Trends Data\n",
    "\n",
    "gtrends = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/google_trends/gtrends_2015-2020_clean.csv')\n",
    "\n",
    "#set dattime\n",
    "gtrends['date'] = pd.to_datetime(gtrends['date'])\n",
    "\n",
    "display(gtrends.shape)\n",
    "\n",
    "#merge text data with gtrends data\n",
    "gtrends_gdelt = gtrends.merge(test_resample, how='inner', left_on = 'date', right_on = 'date_time' )\n",
    "\n",
    "#create boolean mask to drop strings with less than 200 characters\n",
    "gtrends_gdelt['text_bool'] = gtrends_gdelt['text'].apply(lambda x: len(x) < 200)\n",
    "\n",
    "#drop text with less than 200 chracters\n",
    "gtrends_gdelt = gtrends_gdelt[gtrends_gdelt['text_bool']==False]\n",
    "\n",
    "gtrends_gdelt.set_index('date', inplace=True)\n",
    "\n",
    "#filter data to text and target\n",
    "depression_target = gtrends_gdelt[['text', 'depression']]\n",
    "\n",
    "#checking distribution of depression volume \n",
    "depression_target['depression'].plot()\n",
    "\n",
    "depression_target.describe()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(depression_target['depression'])\n",
    "plt.show()\n",
    "\n",
    "#binarize depression target\n",
    "depression_target['d_search_bin'] = np.where(depression_target['depression'] >= 36, 1, 0)\n",
    "\n",
    "#checking distribution of binarized target\n",
    "depression_target['d_search_bin'].value_counts()\n",
    "\n",
    "#setting x, y\n",
    "X = depression_target['text']\n",
    "y = depression_target['d_search_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Bag of words with ngram_range(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23244,), X_test shape: (9963,), y_train shape: (23244,), y_test shape: (9963,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB with TF-IDF and unigam, bigram, and trigram\n",
    "- Test acc: 60.7%\n",
    "- Train acc: 74.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.607447555957041"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit and score test data\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "model.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448373773877129"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score training data\n",
    "model.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with unigram only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23244,), X_test shape: (9963,), y_train shape: (23244,), y_test shape: (9963,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = TfidfVectorizer(min_df=5)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed\n"
     ]
    }
   ],
   "source": [
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB with TF-IDF, unigram\n",
    "\n",
    "- Train acc: 58.2 percent\n",
    "- Test acc: 54.78 percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate and fit model\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "#model.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5819996558251592"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score train\n",
    "model.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5475258456288267"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score test\n",
    "model.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer Bagofwords, unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23244,), X_test shape: (9963,), y_train shape: (23244,), y_test shape: (9963,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = CountVectorizer(min_df=5)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB with CountVect\n",
    "- Train acc: 74 percent\n",
    "- Test acc: 60.7 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.740234038891757"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6069456990866205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "display(model.score(X_train_transformed, y_train))\n",
    "display(model.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer with unigram, bigram, and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23244,), X_test shape: (9963,), y_train shape: (23244,), y_test shape: (9963,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = CountVectorizer(min_df=5, ngram_range=(1,3))\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BernoulliNB with CountVectorizer, unigram, bigram, and trigram\n",
    "- Train acc: 74 percent\n",
    "- Test acc: 60.7 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6069456990866205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.740234038891757"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "display(model.score(X_test_transformed, y_test))\n",
    "display(model.score(X_train_transformed, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer with unigram, bigram, and trigram, and max_features=10000 and StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23244,), X_test shape: (9963,), y_train shape: (23244,), y_test shape: (9963,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = CountVectorizer(min_df=5, ngram_range=(1,3), max_features=10000)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transform with standardscaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X_train_transformed)\n",
    "X_train_transformed = scaler.transform(X_train_transformed)\n",
    "X_test_transformed = scaler.transform(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with CountVect, unigram, bigram, trigram, max_features=10,000 and standard scalling\n",
    "- Train acc: 63.9 percent\n",
    "- Test acc: 59.6 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "model completed\n",
      "fitting model\n",
      "model fitted\n",
      "scoring training data\n",
      "scoring test data\n",
      "Training score: 0.6394768542419549\n",
      "Test score: 0.5956037338151159\n"
     ]
    }
   ],
   "source": [
    "print('creating model')\n",
    "model = LogisticRegression(C=.01, solver='saga', max_iter=10000)\n",
    "print('model completed')\n",
    "\n",
    "\n",
    "#fit model\n",
    "print('fitting model')\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('model fitted')\n",
    "\n",
    "#score training set \n",
    "print('scoring training data')\n",
    "train_score = model.score(X_train_transformed, y_train)\n",
    "\n",
    "#score test set\n",
    "print('scoring test data')\n",
    "test_score = model.score(X_test_transformed, y_test)\n",
    "\n",
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "neptune": {
   "notebookId": "79304995-ce51-467f-9eae-b12339044928"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
