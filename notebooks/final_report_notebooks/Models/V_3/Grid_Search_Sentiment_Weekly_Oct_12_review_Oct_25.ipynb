{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search on daily text data, target = Brandwatch sentiment, no shift, 2018 - 2020\n",
    "\n",
    "- Early effort to predict Brandwatch social media sentiment from daily aggregated articles with no shift\n",
    "- Resample data set was too small, at just only 366 rows\n",
    "- As a result, models badly overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import SparsePCA\n",
    "import spacy\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and shaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/master_data_set/text_with_tokens_52k.csv')\n",
    "#convert date to datetime object\n",
    "text['date'] = pd.to_datetime(text['date'])\n",
    "\n",
    "#create day groupby object\n",
    "grouped_text = text.groupby([text['date'].dt.year, text['date'].dt.month, text['date'].dt.day])\n",
    "\n",
    "#aggregating tokens by day\n",
    "text_day_grouped = grouped_text['text_token'].agg(lambda column: \"\".join(column))\n",
    "\n",
    "#set as df\n",
    "text_day_grouped = pd.DataFrame(text_day_grouped)\n",
    "\n",
    "#rename index\n",
    "text_day_grouped = text_day_grouped.rename_axis(index=['year', 'month', 'day'])\n",
    "\n",
    "#reset_index\n",
    "text_day_grouped = text_day_grouped.reset_index()\n",
    "\n",
    "#create datetime object col\n",
    "text_day_grouped['date_grouped'] = pd.to_datetime(text_day_grouped[['year', 'month', 'day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text_token</th>\n",
       "      <th>date_grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>['answer', 'resounding', 'myriad', 'claim', 'e...</td>\n",
       "      <td>2015-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['scientist', 'center', 'controversy', 'fossil...</td>\n",
       "      <td>2015-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>['scientist', 'step', 'closer', 'understand', ...</td>\n",
       "      <td>2015-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>['high', 'blessed', 'relief', 'finally', 'pres...</td>\n",
       "      <td>2015-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>['california', 'lead', 'nation', 'take', 'acti...</td>\n",
       "      <td>2015-03-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day                                         text_token  \\\n",
       "0  2015      3    2  ['answer', 'resounding', 'myriad', 'claim', 'e...   \n",
       "1  2015      3    3  ['scientist', 'center', 'controversy', 'fossil...   \n",
       "2  2015      3    4  ['scientist', 'step', 'closer', 'understand', ...   \n",
       "3  2015      3    5  ['high', 'blessed', 'relief', 'finally', 'pres...   \n",
       "4  2015      3    6  ['california', 'lead', 'nation', 'take', 'acti...   \n",
       "\n",
       "  date_grouped  \n",
       "0   2015-03-02  \n",
       "1   2015-03-03  \n",
       "2   2015-03-04  \n",
       "3   2015-03-05  \n",
       "4   2015-03-06  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review text data\n",
    "text_day_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in Brandwatch social media sentiment data\n",
    "sentiment = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/brandwatch/bw_sentiment_emotion_day/bw_sentiment_2018-2020.csv')\n",
    "\n",
    "#drop unnecessary columns\n",
    "sentiment.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "#create datetime object column\n",
    "sentiment['days'] = pd.to_datetime(sentiment['days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>-1.119873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-06</td>\n",
       "      <td>-0.847089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>-1.485399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>-0.894346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>-0.762045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        days  sentiment\n",
       "0 2018-10-05  -1.119873\n",
       "1 2018-10-06  -0.847089\n",
       "2 2018-10-07  -1.485399\n",
       "3 2018-10-08  -0.894346\n",
       "4 2018-10-09  -0.762045"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review sentiment data\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarizing sentiment on -1.48 mean value\n",
    "sentiment['binary_sentiment'] = np.where(sentiment['sentiment'] >= -1.52, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge text and target data\n",
    "x_y_complete = sentiment.merge(text_day_grouped, how='inner',  left_on='days', right_on='date_grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>binary_sentiment</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>text_token</th>\n",
       "      <th>date_grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>-1.119873</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>['kuala', 'lumpur', 'oct', '4', 'thomson', 're...</td>\n",
       "      <td>2018-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-06</td>\n",
       "      <td>-0.847089</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>['past', 'couple', 'week', 'see', 'mr.', 'trum...</td>\n",
       "      <td>2018-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-07</td>\n",
       "      <td>-1.485399</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>['couple', 'contact', 'december', '2016', 'was...</td>\n",
       "      <td>2018-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-08</td>\n",
       "      <td>-0.894346</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>['cheltenham', 'england', 'thomson', 'reuters'...</td>\n",
       "      <td>2018-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-09</td>\n",
       "      <td>-0.762045</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>['stockholm', 'reuters', 'americans', 'william...</td>\n",
       "      <td>2018-10-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        days  sentiment  binary_sentiment  year  month  day  \\\n",
       "0 2018-10-05  -1.119873                 1  2018     10    5   \n",
       "1 2018-10-06  -0.847089                 1  2018     10    6   \n",
       "2 2018-10-07  -1.485399                 1  2018     10    7   \n",
       "3 2018-10-08  -0.894346                 1  2018     10    8   \n",
       "4 2018-10-09  -0.762045                 1  2018     10    9   \n",
       "\n",
       "                                          text_token date_grouped  \n",
       "0  ['kuala', 'lumpur', 'oct', '4', 'thomson', 're...   2018-10-05  \n",
       "1  ['past', 'couple', 'week', 'see', 'mr.', 'trum...   2018-10-06  \n",
       "2  ['couple', 'contact', 'december', '2016', 'was...   2018-10-07  \n",
       "3  ['cheltenham', 'england', 'thomson', 'reuters'...   2018-10-08  \n",
       "4  ['stockholm', 'reuters', 'americans', 'william...   2018-10-09  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review merged data\n",
    "x_y_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set x and y\n",
    "X = x_y_complete['text_token']\n",
    "y = x_y_complete['binary_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with Logistic Regression\n",
    "\n",
    "- Training score: 0.7324675324675325\n",
    "- Test score: 0.6566265060240963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (385,), X_test shape: (166,), y_train shape: (385,), y_test shape: (166,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n",
      "creating model\n",
      "model completed\n",
      "fitting model\n",
      "model fitted\n",
      "scoring training data\n",
      "scoring test data\n",
      "Training score: 0.7324675324675325\n",
      "Test score: 0.6566265060240963\n"
     ]
    }
   ],
   "source": [
    "#test train split\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.3, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = TfidfVectorizer(min_df=5)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')\n",
    "\n",
    "#create model\n",
    "print('creating model')\n",
    "model = LogisticRegression(C=.1, solver='liblinear')\n",
    "print('model completed')\n",
    "\n",
    "\n",
    "#fit model\n",
    "print('fitting model')\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('model fitted')\n",
    "\n",
    "#score training set \n",
    "print('scoring training data')\n",
    "train_score = model.score(X_train_transformed, y_train)\n",
    "\n",
    "#score test set\n",
    "print('scoring test data')\n",
    "test_score = model.score(X_test_transformed, y_test)\n",
    "\n",
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "#return (bagofwords, model, X_train_transformed, X_test_transformed, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with TF-IDF data, models = Logistic Regression, Random Forest, SVC, SGDClassifier\n",
    "\n",
    "- Best model - SGDClassifier\n",
    "- Train score: 100 percent\n",
    "- Test score: 67 percent \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 73 candidates, totalling 365 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 365 out of 365 | elapsed:  4.2min finished\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "estimators = [('model', LogisticRegression())]\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "param_grid = [{'model': [LogisticRegression()],\n",
    " \n",
    "             'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "             'model__solver': ['liblinear', 'newton-cg', 'sag', 'saga','lbfgs']},\\\n",
    "              \n",
    "             {'model': [SVC()],\n",
    "            'model__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'model__C': [0.001, 0.01, 0.1, 1, 10]},\\\n",
    "              \n",
    "             {'model': [RandomForestClassifier()]},\n",
    "              \n",
    "             {'model': [SGDClassifier()],\n",
    "            'model__alpha': (0.00001, 0.000001),\n",
    "            'model__penalty': ('l2', 'elasticnet'),\n",
    "            'model__max_iter': (10, 50, 80)}]\n",
    "                       \n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "fittedgrid = grid.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score training data\n",
    "fittedgrid.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6686746987951807"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score test data\n",
    "fittedgrid.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/tmp/tmpdkl336el',\n",
       "         steps=[('model',\n",
       "                 SGDClassifier(alpha=1e-05, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=50, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='elasticnet', power_t=0.5,\n",
       "                               random_state=None, shuffle=True, tol=0.001,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review best model (which was not very good)\n",
    "fittedgrid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search with TF-IDF data, models = Logistic Regression, Random Forest, SVC, SGDClassifier, and XGBoost\n",
    "\n",
    "- Added XGBoost to the mix\n",
    "- Best model - Logistic Regression\n",
    "- Train score: 98.7 percent\n",
    "- Test score: 67 percent \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 223 candidates, totalling 1115 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed: 30.7min\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 41.0min\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed: 46.6min\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed: 50.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 54.9min\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed: 60.3min\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed: 65.5min\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed: 69.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 71.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed: 75.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed: 79.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1115 out of 1115 | elapsed: 82.5min finished\n"
     ]
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "estimators = [('model', LogisticRegression())]\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "param_grid = [{'model': [LogisticRegression()],\n",
    " \n",
    "             'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "             'model__solver': ['liblinear', 'newton-cg', 'sag', 'saga','lbfgs']},\\\n",
    "              \n",
    "             {'model': [SVC()],\n",
    "            'model__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'model__C': [0.001, 0.01, 0.1, 1, 10]},\\\n",
    "              \n",
    "             {'model': [RandomForestClassifier()]},\n",
    "              \n",
    "             {'model': [SGDClassifier()],\n",
    "            'model__alpha': (0.00001, 0.000001),\n",
    "            'model__penalty': ('l2', 'elasticnet'),\n",
    "            'model__max_iter': (10, 50, 80)},\n",
    "              \n",
    "            {'model': [XGBClassifier(n_jobs=-1)],\n",
    "              'model__n_estimators': np.arange(1,500,10),\n",
    "             'model__learning_rate': [0.25, 0.5, 1]}]\n",
    "                       \n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "fittedgrid = grid.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987012987012987"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score train\n",
    "fittedgrid.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6686746987951807"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score test\n",
    "fittedgrid.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/tmp/tmp_iffnu51',\n",
       "         steps=[('model',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get best model\n",
    "fittedgrid.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "neptune": {
   "notebookId": "cb23e907-9cdf-4865-86c8-7173aeb4676a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
