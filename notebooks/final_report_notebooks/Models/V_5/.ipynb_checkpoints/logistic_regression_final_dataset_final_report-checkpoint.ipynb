{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building jupyterlab assets (build:prod:minimize)\n",
      "Building jupyterlab assets (build:prod:minimize)\n",
      "Building jupyterlab assets (build:prod:minimize)\n",
      "Building jupyterlab assets (build:prod:minimize)\n"
     ]
    }
   ],
   "source": [
    "#installing jupyter lab extensions I like to use\n",
    "!jupyter labextension install @jupyterlab/toc;\n",
    "!jupyter labextension install @ijmbarr/jupyterlab_spellchecker;\n",
    "!jupyter labextension install @aquirdturtle/collapsible_headings;\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.18.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/site-packages (from spacy) (4.46.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy) (46.2.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing and importing spacy, which I use for tokenizing text\n",
    "!pip install spacy;\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 714 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.2.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12011738 sha256=2025bde9fd5870ab410d9b525f7e1a6d3ebd12066cbcd24c9e4f01f91bbde7b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ym2q6i0i/wheels/51/19/da/a3885266a3c241aff0ad2eb674ae058fd34a4870fef1c0a5a0\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#importing spacy's tokenizer language model to support slightly more advanced tokenizing\n",
    "!python -m spacy download en_core_web_sm;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads spacy's small language model for tokenizer, disabling parameters not needed to speed up performance\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer helper function, that runs as part of data processing cell\n",
    "def tokenizer(text, nlp):\n",
    "\n",
    "    token_list = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.is_punct==False:\n",
    "            if token.text != ' ':\n",
    "                token_list.append((token.lemma_).lower())\n",
    "    str_tokens = ' '.join(token_list)\n",
    "    return str_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and shaping data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in text data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gdelt_df_shape: (214706, 12)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DF HEAD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>GKGRECORDID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>title</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>act_ref_density</th>\n",
       "      <th>self_group_density</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20150302100000-674</td>\n",
       "      <td>2015-03-02 10:00:00</td>\n",
       "      <td>america clean energy laggard</td>\n",
       "      <td>answer resound myriad claim energy need debunk...</td>\n",
       "      <td>0.350631</td>\n",
       "      <td>2.734923</td>\n",
       "      <td>2.384292</td>\n",
       "      <td>5.119215</td>\n",
       "      <td>16.760168</td>\n",
       "      <td>0.420757</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20150302153000-229</td>\n",
       "      <td>2015-03-02 15:30:00</td>\n",
       "      <td>watch meet press treat climate change big joke</td>\n",
       "      <td>hear sen. james inhofe r okla astonishingly ch...</td>\n",
       "      <td>-0.952381</td>\n",
       "      <td>3.492063</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>7.936508</td>\n",
       "      <td>26.984127</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20150302163000-237</td>\n",
       "      <td>2015-03-02 16:30:00</td>\n",
       "      <td>no_title</td>\n",
       "      <td>mary bowerman usa today network visitors show ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>3.628118</td>\n",
       "      <td>25.396825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20150302180000-1352</td>\n",
       "      <td>2015-03-02 18:00:00</td>\n",
       "      <td>russian energy deal comes contentious time</td>\n",
       "      <td>mr. fridman business track record hard cameron...</td>\n",
       "      <td>-1.147541</td>\n",
       "      <td>1.803279</td>\n",
       "      <td>2.950820</td>\n",
       "      <td>4.754098</td>\n",
       "      <td>19.508197</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20150302203000-163</td>\n",
       "      <td>2015-03-02 20:30:00</td>\n",
       "      <td>climate change cause syrian civil war</td>\n",
       "      <td>climate change spark historic drought syria co...</td>\n",
       "      <td>-8.054523</td>\n",
       "      <td>0.371747</td>\n",
       "      <td>8.426270</td>\n",
       "      <td>8.798017</td>\n",
       "      <td>24.039653</td>\n",
       "      <td>0.247831</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          GKGRECORDID                 DATE  \\\n",
       "0           0   20150302100000-674  2015-03-02 10:00:00   \n",
       "1           1   20150302153000-229  2015-03-02 15:30:00   \n",
       "2           2   20150302163000-237  2015-03-02 16:30:00   \n",
       "3           3  20150302180000-1352  2015-03-02 18:00:00   \n",
       "4           4   20150302203000-163  2015-03-02 20:30:00   \n",
       "\n",
       "                                            title  \\\n",
       "0                    america clean energy laggard   \n",
       "1  watch meet press treat climate change big joke   \n",
       "2                                        no_title   \n",
       "3      russian energy deal comes contentious time   \n",
       "4           climate change cause syrian civil war   \n",
       "\n",
       "                                         text_tokens  avg_tone  pos_words  \\\n",
       "0  answer resound myriad claim energy need debunk...  0.350631   2.734923   \n",
       "1  hear sen. james inhofe r okla astonishingly ch... -0.952381   3.492063   \n",
       "2  mary bowerman usa today network visitors show ...  0.000000   1.814059   \n",
       "3  mr. fridman business track record hard cameron... -1.147541   1.803279   \n",
       "4  climate change spark historic drought syria co... -8.054523   0.371747   \n",
       "\n",
       "   neg_words  polarity  act_ref_density  self_group_density  word_count  \n",
       "0   2.384292  5.119215        16.760168            0.420757        1186  \n",
       "1   4.444444  7.936508        26.984127            1.428571         576  \n",
       "2   1.814059  3.628118        25.396825            0.000000         405  \n",
       "3   2.950820  4.754098        19.508197            0.409836        1119  \n",
       "4   8.426270  8.798017        24.039653            0.247831         743  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "setting datatime\n",
      "\n",
      "\n",
      "Filling empty title columns\n",
      "\n",
      "\n",
      "tokenizing titles\n",
      "\n",
      "\n",
      "joining text and title\n",
      "Reshaped GDELT DF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df: (214706, 14)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "resampling at 1h\n",
      "\n",
      "\n",
      "GDELT Text Resampled by 1 hour\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-02 10:00:00</th>\n",
       "      <td>america clean energy laggardanswer resound myr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-02 11:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-02 12:00:00</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            title_text\n",
       "date_time                                                             \n",
       "2015-03-02 10:00:00  america clean energy laggardanswer resound myr...\n",
       "2015-03-02 11:00:00                                                   \n",
       "2015-03-02 12:00:00                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Getting GDELT tonal scores and resampling to 1h by calculating mean\n",
      "\n",
      "\n",
      "GDELT Tonal Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>act_ref_density</th>\n",
       "      <th>self_group_density</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-03-02 10:00:00</th>\n",
       "      <td>0.350631</td>\n",
       "      <td>2.734923</td>\n",
       "      <td>2.384292</td>\n",
       "      <td>5.119215</td>\n",
       "      <td>16.760168</td>\n",
       "      <td>0.420757</td>\n",
       "      <td>1186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-02 11:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-02 12:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     avg_tone  pos_words  neg_words  polarity  \\\n",
       "date_time                                                       \n",
       "2015-03-02 10:00:00  0.350631   2.734923   2.384292  5.119215   \n",
       "2015-03-02 11:00:00       NaN        NaN        NaN       NaN   \n",
       "2015-03-02 12:00:00       NaN        NaN        NaN       NaN   \n",
       "\n",
       "                     act_ref_density  self_group_density  word_count  \n",
       "date_time                                                             \n",
       "2015-03-02 10:00:00        16.760168            0.420757      1186.0  \n",
       "2015-03-02 11:00:00              NaN                 NaN         NaN  \n",
       "2015-03-02 12:00:00              NaN                 NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Joining resampled text data with resampled tonal data\n",
      "Text and Tonal resampled GDELT data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>title_text</th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>act_ref_density</th>\n",
       "      <th>self_group_density</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-02 10:00:00</td>\n",
       "      <td>america clean energy laggardanswer resound myr...</td>\n",
       "      <td>0.350631</td>\n",
       "      <td>2.734923</td>\n",
       "      <td>2.384292</td>\n",
       "      <td>5.119215</td>\n",
       "      <td>16.760168</td>\n",
       "      <td>0.420757</td>\n",
       "      <td>1186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-03-02 11:00:00</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03-02 12:00:00</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time                                         title_text  \\\n",
       "0 2015-03-02 10:00:00  america clean energy laggardanswer resound myr...   \n",
       "1 2015-03-02 11:00:00                                                      \n",
       "2 2015-03-02 12:00:00                                                      \n",
       "\n",
       "   avg_tone  pos_words  neg_words  polarity  act_ref_density  \\\n",
       "0  0.350631   2.734923   2.384292  5.119215        16.760168   \n",
       "1       NaN        NaN        NaN       NaN              NaN   \n",
       "2       NaN        NaN        NaN       NaN              NaN   \n",
       "\n",
       "   self_group_density  word_count  \n",
       "0            0.420757      1186.0  \n",
       "1                 NaN         NaN  \n",
       "2                 NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of new tonal text GDELT data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'clean text shape: (49115, 9)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checking for empty strings in text feature\n",
      "\n",
      "\n",
      "Displaying number of empty strings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'text > 0: 29822'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Adding padding to text to ensure complete join with Google Trends Target Data\n",
      "\n",
      "\n",
      "appending new padding to test_resample\n",
      "\n",
      "\n",
      "Reviewing reshaped GDELT text and tonal feature set\n",
      "\n",
      "\n",
      "Reading in google trends\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gtrend shape: (66741, 8)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouping gtrends data and removing duplicates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gtrends duplicates removed: (50096, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing extra google trends data\n",
      "setting datetime on gtrends\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gtrends timeframe reduced: (48538, 6)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gtrends_gdelt shape: (48528, 14)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"gtrends_gdelt cols: Index(['date', 'depression', 'anxiety', 'government', 'politics', 'democracy',\\n       'title_text', 'avg_tone', 'pos_words', 'neg_words', 'polarity',\\n       'act_ref_density', 'self_group_density', 'word_count'],\\n      dtype='object')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtrends_gdelt complete records: 29441\n",
      "binary_depression value counts: 1    24881\n",
      "0    23647\n",
      "Name: depression_binary, dtype: int64\n",
      "shifting google trends by -24 houors\n",
      "setting final df\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29389 entries, 0 to 48475\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   date                29389 non-null  datetime64[ns]\n",
      " 1   depression          29389 non-null  float64       \n",
      " 2   anxiety             29389 non-null  float64       \n",
      " 3   government          29389 non-null  float64       \n",
      " 4   politics            29389 non-null  float64       \n",
      " 5   democracy           29389 non-null  float64       \n",
      " 6   title_text          29389 non-null  object        \n",
      " 7   avg_tone            29389 non-null  float64       \n",
      " 8   pos_words           29389 non-null  float64       \n",
      " 9   neg_words           29389 non-null  float64       \n",
      " 10  polarity            29389 non-null  float64       \n",
      " 11  act_ref_density     29389 non-null  float64       \n",
      " 12  self_group_density  29389 non-null  float64       \n",
      " 13  word_count          29389 non-null  float64       \n",
      " 14  depression_binary   29389 non-null  int64         \n",
      " 15  shifted_12h         29389 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(13), int64(1), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>depression</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>government</th>\n",
       "      <th>politics</th>\n",
       "      <th>democracy</th>\n",
       "      <th>title_text</th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>act_ref_density</th>\n",
       "      <th>self_group_density</th>\n",
       "      <th>word_count</th>\n",
       "      <th>depression_binary</th>\n",
       "      <th>shifted_12h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-02 10:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>america clean energy laggardanswer resound myr...</td>\n",
       "      <td>0.350631</td>\n",
       "      <td>2.734923</td>\n",
       "      <td>2.384292</td>\n",
       "      <td>5.119215</td>\n",
       "      <td>16.760168</td>\n",
       "      <td>0.420757</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-03-02 15:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>watch meet press treat climate change big joke...</td>\n",
       "      <td>-0.952381</td>\n",
       "      <td>3.492063</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>7.936508</td>\n",
       "      <td>26.984127</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-03-02 16:00:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>no_titlemary bowerman usa today network visito...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>3.628118</td>\n",
       "      <td>25.396825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-03-02 18:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>russian energy deal come contentious timemr. f...</td>\n",
       "      <td>-1.147541</td>\n",
       "      <td>1.803279</td>\n",
       "      <td>2.950820</td>\n",
       "      <td>4.754098</td>\n",
       "      <td>19.508197</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>1119.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-03-02 20:00:00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>climate change cause syrian civil warclimate c...</td>\n",
       "      <td>-5.368873</td>\n",
       "      <td>0.909366</td>\n",
       "      <td>6.278239</td>\n",
       "      <td>7.187605</td>\n",
       "      <td>24.159231</td>\n",
       "      <td>0.123712</td>\n",
       "      <td>616.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  depression  anxiety  government  politics  democracy  \\\n",
       "0  2015-03-02 10:00:00        15.0     16.0        71.0       4.0        2.0   \n",
       "5  2015-03-02 15:00:00        18.0     13.0        42.0       6.0        5.0   \n",
       "6  2015-03-02 16:00:00        17.0     13.0        41.0       6.0        5.0   \n",
       "8  2015-03-02 18:00:00        18.0     14.0        39.0       6.0        4.0   \n",
       "10 2015-03-02 20:00:00        17.0     13.0        35.0       5.0        4.0   \n",
       "\n",
       "                                           title_text  avg_tone  pos_words  \\\n",
       "0   america clean energy laggardanswer resound myr...  0.350631   2.734923   \n",
       "5   watch meet press treat climate change big joke... -0.952381   3.492063   \n",
       "6   no_titlemary bowerman usa today network visito...  0.000000   1.814059   \n",
       "8   russian energy deal come contentious timemr. f... -1.147541   1.803279   \n",
       "10  climate change cause syrian civil warclimate c... -5.368873   0.909366   \n",
       "\n",
       "    neg_words  polarity  act_ref_density  self_group_density   word_count  \\\n",
       "0    2.384292  5.119215        16.760168            0.420757  1186.000000   \n",
       "5    4.444444  7.936508        26.984127            1.428571   576.000000   \n",
       "6    1.814059  3.628118        25.396825            0.000000   405.000000   \n",
       "8    2.950820  4.754098        19.508197            0.409836  1119.000000   \n",
       "10   6.278239  7.187605        24.159231            0.123712   616.333333   \n",
       "\n",
       "    depression_binary  shifted_12h  \n",
       "0                   0          0.0  \n",
       "5                   0          0.0  \n",
       "6                   0          0.0  \n",
       "8                   0          0.0  \n",
       "10                  0          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_text</th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>pos_words</th>\n",
       "      <th>neg_words</th>\n",
       "      <th>polarity</th>\n",
       "      <th>act_ref_density</th>\n",
       "      <th>self_group_density</th>\n",
       "      <th>word_count</th>\n",
       "      <th>shifted_12h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>america clean energy laggardanswer resound myr...</td>\n",
       "      <td>0.350631</td>\n",
       "      <td>2.734923</td>\n",
       "      <td>2.384292</td>\n",
       "      <td>5.119215</td>\n",
       "      <td>16.760168</td>\n",
       "      <td>0.420757</td>\n",
       "      <td>1186.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>watch meet press treat climate change big joke...</td>\n",
       "      <td>-0.952381</td>\n",
       "      <td>3.492063</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>7.936508</td>\n",
       "      <td>26.984127</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no_titlemary bowerman usa today network visito...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>1.814059</td>\n",
       "      <td>3.628118</td>\n",
       "      <td>25.396825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>russian energy deal come contentious timemr. f...</td>\n",
       "      <td>-1.147541</td>\n",
       "      <td>1.803279</td>\n",
       "      <td>2.950820</td>\n",
       "      <td>4.754098</td>\n",
       "      <td>19.508197</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>1119.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>climate change cause syrian civil warclimate c...</td>\n",
       "      <td>-5.368873</td>\n",
       "      <td>0.909366</td>\n",
       "      <td>6.278239</td>\n",
       "      <td>7.187605</td>\n",
       "      <td>24.159231</td>\n",
       "      <td>0.123712</td>\n",
       "      <td>616.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title_text  avg_tone  pos_words  \\\n",
       "0   america clean energy laggardanswer resound myr...  0.350631   2.734923   \n",
       "5   watch meet press treat climate change big joke... -0.952381   3.492063   \n",
       "6   no_titlemary bowerman usa today network visito...  0.000000   1.814059   \n",
       "8   russian energy deal come contentious timemr. f... -1.147541   1.803279   \n",
       "10  climate change cause syrian civil warclimate c... -5.368873   0.909366   \n",
       "\n",
       "    neg_words  polarity  act_ref_density  self_group_density   word_count  \\\n",
       "0    2.384292  5.119215        16.760168            0.420757  1186.000000   \n",
       "5    4.444444  7.936508        26.984127            1.428571   576.000000   \n",
       "6    1.814059  3.628118        25.396825            0.000000   405.000000   \n",
       "8    2.950820  4.754098        19.508197            0.409836  1119.000000   \n",
       "10   6.278239  7.187605        24.159231            0.123712   616.333333   \n",
       "\n",
       "    shifted_12h  \n",
       "0           0.0  \n",
       "5           0.0  \n",
       "6           0.0  \n",
       "8           0.0  \n",
       "10          0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in text data from GDELT\n",
    "print('reading in text data')\n",
    "df = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/master_data_set/gdelt_text_tone_complete_oct_22.csv')\n",
    "\n",
    "display(f'gdelt_df_shape: {df.shape}')\n",
    "print('\\n')\n",
    "\n",
    "print('DF HEAD')\n",
    "display(df.head())\n",
    "\n",
    "print('\\n')\n",
    "print('setting datatime')\n",
    "\n",
    "#converts datetime\n",
    "df['date_time'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "#sets datetime index\n",
    "df.set_index('date_time', inplace=True)\n",
    "\n",
    "print('\\n')\n",
    "print('Filling empty title columns')\n",
    "\n",
    "#fills missing title with no_title\n",
    "df['title'].fillna('no_title', inplace = True)\n",
    "\n",
    "print('\\n')\n",
    "print('tokenizing titles')\n",
    "\n",
    "#tokenzies title\n",
    "df['title_tokens'] = df['title'].apply(lambda x: tokenizer(str(x), nlp))\n",
    "\n",
    "print('\\n')\n",
    "print('joining text and title')\n",
    "\n",
    "#joins title and text\n",
    "df['title_text'] = df['title_tokens'] + df['text_tokens']\n",
    "\n",
    "#display(df.head())\n",
    "print('Reshaped GDELT DF')\n",
    "display(f'df: {df.shape}')\n",
    "\n",
    "print('\\n')\n",
    "print('resampling at 1h')\n",
    "\n",
    "#resample data into 1 hour increments. Joins articles into one giant string for each hour\n",
    "test_resample = df.resample('1h')['title_text'].agg(lambda column: \"\".join(column))\n",
    "#convert to dataframe\n",
    "test_resample = pd.DataFrame(test_resample)\n",
    "\n",
    "print('\\n')\n",
    "print('GDELT Text Resampled by 1 hour')\n",
    "display(test_resample.head(3))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Getting GDELT tonal scores and resampling to 1h by calculating mean')\n",
    "\n",
    "#grabs scores (I had aspirations of using this data in my model)\n",
    "scores = df[['avg_tone', 'pos_words', 'neg_words', 'polarity', 'act_ref_density', 'self_group_density', 'word_count']]\n",
    "\n",
    "#resample scores metadata\n",
    "scores = scores.resample('1h').mean()\n",
    "\n",
    "print('\\n')\n",
    "print('GDELT Tonal Dataset')\n",
    "display(scores.head(3))\n",
    "print('\\n')\n",
    "\n",
    "#Joining resampled text data with resampled tonal data\n",
    "print('Joining resampled text data with resampled tonal data')\n",
    "test_resample = pd.concat([test_resample, scores], axis=1)\n",
    "\n",
    "#resetting index\n",
    "test_resample.reset_index(inplace=True)\n",
    "\n",
    "#display text and tonal resampled data set\n",
    "print('Text and Tonal resampled GDELT data')\n",
    "display(test_resample.head(3))\n",
    "print('\\n')\n",
    "\n",
    "print('Shape of new tonal text GDELT data')\n",
    "display(f'clean text shape: {test_resample.shape}')\n",
    "\n",
    "#checking for empty title_text strings. Can't use null here because elements in text are a string, even empty string\n",
    "print('\\n')\n",
    "print('Checking for empty strings in text feature')\n",
    "useful_sample_size = (test_resample['title_text'].str.len()>0).sum()\n",
    "\n",
    "print('\\n')\n",
    "print('Displaying number of empty strings')\n",
    "display(f'text > 0: {useful_sample_size}')\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "#Adding padding to text to ensure complete join with Google Trends Target Data\n",
    "#this is important when I `shift` the Google Trends target data\n",
    "#Empty strings will eventually be dropped\n",
    "print('Adding padding to text to ensure complete join with Google Trends Target Data')\n",
    "\n",
    "#getting column headers\n",
    "columns = list(test_resample.columns)\n",
    "\n",
    "#creating list of data for padding\n",
    "date_list = list(pd.date_range(start='2020-10-07 21:00:00', end='2020-10-11 00:00:00', freq='1h'))\n",
    "\n",
    "#creating dict of columns with date_list and empty_string\n",
    "data = {'date_time': date_list, 'title_text': 'empty_string'}\n",
    "\n",
    "#creatinge padding dataframe\n",
    "df_holder = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "#appending padding dataframe to text dataframe\n",
    "print('\\n')\n",
    "print('appending new padding to test_resample')\n",
    "test_resample = test_resample.append(df_holder)\n",
    "\n",
    "print('\\n')\n",
    "print('Reviewing reshaped GDELT text and tonal feature set')\n",
    "print('\\n')\n",
    "\n",
    "#read in gtrend target data\n",
    "print('Reading in google trends')\n",
    "\n",
    "gtrends = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/google_trends/gtrends_2015-2020_clean.csv')\n",
    "\n",
    "print('\\n')\n",
    "display(f'gtrend shape: {gtrends.shape}')\n",
    "\n",
    "print('grouping gtrends data and removing duplicates')\n",
    "gtrends = gtrends.groupby('date').mean()\n",
    "\n",
    "display(f'gtrends duplicates removed: {gtrends.shape}')\n",
    "\n",
    "print('removing extra google trends data')\n",
    "\n",
    "#returning only Gtrends data that fits in time range of text data\n",
    "gtrends = gtrends['2015-03-02 00:00:00': '2020-10-10 00:00:00']\n",
    "\n",
    "#reset index\n",
    "gtrends.reset_index(inplace=True)\n",
    "\n",
    "#drop unneeded columns\n",
    "gtrends.drop(['Unnamed: 0', 'isPartial'], axis=1, inplace=True)\n",
    "\n",
    "print('setting datetime on gtrends')\n",
    "\n",
    "#convert date col to datetime\n",
    "gtrends['date'] = pd.to_datetime(gtrends['date'])\n",
    "\n",
    "display(f'gtrends timeframe reduced: {gtrends.shape}')\n",
    "\n",
    "#merging text and trends\n",
    "gtrends_gdelt = gtrends.merge(test_resample, how='inner', left_on = 'date', right_on = 'date_time')\n",
    "\n",
    "#dropping extra data column\n",
    "gtrends_gdelt.drop('date_time', axis=1, inplace=True)\n",
    "display(f'gtrends_gdelt shape: {gtrends_gdelt.shape}')\n",
    "display(f'gtrends_gdelt cols: {gtrends_gdelt.columns}')\n",
    "\n",
    "#checking complete records\n",
    "complete_records = (gtrends_gdelt['title_text'].str.len()>0).sum()\n",
    "print(f'gtrends_gdelt complete records: {complete_records}')\n",
    "\n",
    "#binarizing depression\n",
    "gtrends_gdelt['depression_binary'] = np.where(gtrends_gdelt['depression'] >= 36, 1, 0)\n",
    "\n",
    "#grabbing value counts to ensure they are balanced\n",
    "dep_val_count = gtrends_gdelt['depression_binary'].value_counts()\n",
    "\n",
    "print(f'binary_depression value counts: {dep_val_count}')\n",
    "\n",
    "#shifting trends data to so target is now 12 hours later that text feature data\n",
    "print('shifting google trends by -12 houors')\n",
    "gtrends_gdelt['shifted_12h'] = gtrends_gdelt['depression_binary'].shift(-12)\n",
    "\n",
    "#dropping NA rows, which should just be 24 end rows\n",
    "gtrends_gdelt.dropna(inplace=True)\n",
    "\n",
    "#grapping complete rows\n",
    "\n",
    "print('setting final df')\n",
    "\n",
    "#removing rows that don't have article text data\n",
    "gtrends_text_final = gtrends_gdelt[gtrends_gdelt['title_text'].str.len()>12]\n",
    "\n",
    "print(gtrends_text_final.info())\n",
    "print('\\n')\n",
    "display(gtrends_text_final.head())\n",
    "print('\\n')\n",
    "\n",
    "#creating final dataset for machine learning. Keeping many of these columns because of aspriation hope that I can test in my mod\n",
    "x_y = gtrends_text_final[['title_text', 'avg_tone', 'pos_words', 'neg_words', 'polarity', 'act_ref_density', 'self_group_density', 'word_count', 'shifted_12h']]\n",
    "x_y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtrends_text_final.to_csv('final_aggregated_data_set_Oct_26.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23511,), X_test shape: (5878,), y_train shape: (23511,), y_test shape: (5878,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#setting x y\n",
    "X = x_y['title_text']\n",
    "y = x_y['shifted_12h']\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = TfidfVectorizer(min_df=5)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF, Google Trends Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done - X_train shape: (23511,), X_test shape: (5878,), y_train shape: (23511,), y_test shape: (5878,)\n",
      "vectorizer done\n",
      "beginng vectorizer fitting\n",
      "vectorizer fitting complete\n",
      "beginning transformation\n",
      "X_train transformed\n",
      "X_test_transformed\n",
      "creating model\n",
      "model completed\n",
      "fitting model\n",
      "model fitted\n",
      "scoring training data\n",
      "scoring test data\n",
      "Training score: 0.6719407936710476\n",
      "Test score: 0.6097312010888057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create model\n",
    "print('creating model')\n",
    "model = LogisticRegression(C=.1, solver='liblinear')\n",
    "print('model completed')\n",
    "\n",
    "\n",
    "#fit model\n",
    "print('fitting model')\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('model fitted')\n",
    "\n",
    "#score training set \n",
    "print('scoring training data')\n",
    "train_score = model.score(X_train_transformed, y_train)\n",
    "\n",
    "#score test set\n",
    "print('scoring test data')\n",
    "test_score = model.score(X_test_transformed, y_test)\n",
    "\n",
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with Sklearn, TF-IDF, Models - LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  75 | elapsed:  1.6min remaining:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9427076687507975"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.631337189520245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/tmp/tmp4v1paivv',\n",
       "         steps=[('model',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'model__C': 10,\n",
       " 'model__solver': 'liblinear'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6414018969843903"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'model': [LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)],\n",
       "  'model__C': [0.001, 0.01, 0.1, 1, 10],\n",
       "  'model__solver': ['liblinear', 'newton-cg', 'sag', 'saga', 'lbfgs']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "estimators = [('model', LogisticRegression())]\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "param_grid = [{'model': [LogisticRegression()],\n",
    " \n",
    "             'model__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "             'model__solver': ['liblinear', 'newton-cg', 'sag', 'saga','lbfgs']}]\n",
    "              \n",
    "#              {'model': [SVC()],\n",
    "#             'model__gamma': [0.001, 0.01, 0.1, 1, 10],\n",
    "#              'model__C': [0.001, 0.01, 0.1, 1, 10]},\\\n",
    "              \n",
    "#              {'model': [RandomForestClassifier()]},\n",
    "              \n",
    "#              {'model': [SGDClassifier()],\n",
    "#             'model__alpha': (0.00001, 0.000001),\n",
    "#             'model__penalty': ('l2', 'elasticnet'),\n",
    "#             'model__max_iter': (10, 50, 80)}]\n",
    "              \n",
    "#             {'model': [XGBClassifier(n_jobs=-1)],\n",
    "#               'model__n_estimators': np.arange(1,600,200),\n",
    "#              'model__learning_rate': [0.25, 0.5, 1]}]\n",
    "                       \n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, n_jobs=-1, cv=3, verbose=10)\n",
    "\n",
    "fittedgrid = grid.fit(X_train_transformed, y_train)\n",
    "\n",
    "display(fittedgrid.score(X_train_transformed, y_train))\n",
    "\n",
    "display(fittedgrid.score(X_test_transformed, y_test))\n",
    "\n",
    "display(fittedgrid.best_estimator_)\n",
    "\n",
    "display(fittedgrid.best_params_)\n",
    "\n",
    "display(fittedgrid.best_score_)\n",
    "\n",
    "display(fittedgrid.param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Grid Search' with H2O Auto ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/site-packages (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting colorama>=0.3.8\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: colorama\n",
      "Successfully installed colorama-0.4.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in links: http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html\n",
      "\u001b[33mWARNING: The repository located at h2o-release.s3.amazonaws.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host h2o-release.s3.amazonaws.com'.\u001b[0m\n",
      "Collecting h2o\n",
      "  Downloading h2o-3.30.1.3.tar.gz (129.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 129.4 MB 35 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from h2o) (2.23.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/site-packages (from h2o) (0.8.7)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from h2o) (0.18.2)\n",
      "Requirement already satisfied: colorama>=0.3.8 in /usr/local/lib/python3.7/site-packages (from h2o) (0.4.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->h2o) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->h2o) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->h2o) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->h2o) (2.9)\n",
      "Building wheels for collected packages: h2o\n",
      "  Building wheel for h2o (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for h2o: filename=h2o-3.30.1.3-py2.py3-none-any.whl size=129446676 sha256=a2b21d6f5e4da4f2f3bf99fde0327b89b5c7f70490e076cfb1cc542c31e21413\n",
      "  Stored in directory: /root/.cache/pip/wheels/c4/08/f0/1b00ab7f2ec7e81f6d51890e1f37665c06d7e32a31e69bcf50\n",
      "Successfully built h2o\n",
      "Installing collected packages: h2o\n",
      "Successfully installed h2o-3.30.1.3\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.7\" 2020-04-14; OpenJDK Runtime Environment (build 11.0.7+10-post-Ubuntu-2ubuntu218.04); OpenJDK 64-Bit Server VM (build 11.0.7+10-post-Ubuntu-2ubuntu218.04, mixed mode)\n",
      "  Starting server from /usr/local/lib/python3.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp_zwyeet0\n",
      "  JVM stdout: /tmp/tmp_zwyeet0/h2o_unknownUser_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp_zwyeet0/h2o_unknownUser_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.30.1.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>27 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_unknownUser_g29jge</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>7.381 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.7.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         03 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.30.1.3\n",
       "H2O_cluster_version_age:    27 days\n",
       "H2O_cluster_name:           H2O_from_python_unknownUser_g29jge\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    7.381 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.7.7 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#H2O specific imports\n",
    "!pip install requests;\n",
    "!pip install tabulate;\n",
    "!pip install \"colorama>=0.3.8\";\n",
    "!pip install future;\n",
    "\n",
    "!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o;\n",
    "\n",
    "import h2o\n",
    "from scipy import sparse\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████"
     ]
    }
   ],
   "source": [
    "#preparing data for H2O\n",
    "train = sparse.hstack((X_train_transformed, np.array(y_train)[:,None]))\n",
    "test = sparse.hstack((X_test_transformed, np.array(y_test)[:,None]))\n",
    "\n",
    "train_h2o = h2o.H2OFrame(train)\n",
    "test_h2o = h2o.H2OFrame(test)\n",
    "\n",
    "y_train_h2o = train_h2o.col_names[-1]\n",
    "X_train_h2o = train_h2o.col_names[:33686]\n",
    "y_test_h2o = test_h2o.col_names[-1]\n",
    "X_test_h2o = test_h2o.col_names[:33686]\n",
    "train_h2o[y_train_h2o] = train_h2o[y_train_h2o].asfactor()\n",
    "test_h2o[y_train_h2o] = test_h2o[y_train_h2o].asfactor()\n",
    "\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "aml = H2OAutoML(max_models=10, max_runtime_secs=500, balance_classes=True)\n",
    "aml.train(x=X_train_h2o, y=y_train_h2o, training_frame=train_h2o)\n",
    "\n",
    "lb = aml.leaderboard\n",
    "\n",
    "lb.head(rows=lb.nrows)\n",
    "\n",
    "display(aml.leader)\n",
    "\n",
    "perf = aml.leader.model_performance(test_h2o)\n",
    "\n",
    "perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP - Article with Binary Polarity Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in gdelt master\n",
    "print('reading in text daya')\n",
    "df = pd.read_csv('/floyd/home/Capstone/cap_notebooks/data/master_data_set/gdelt_text_tone_complete_oct_22.csv')\n",
    "\n",
    "display(f'df: {df.shape}')\n",
    "\n",
    "#display(df.head())\n",
    "\n",
    "print('setting datatime')\n",
    "#cleans date time\n",
    "#df['date_time']  = df['gkgcode'].apply(lambda x: x[:14])\n",
    "\n",
    "\n",
    "\n",
    "#converts datetime\n",
    "df['date_time'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "#sets datetime index\n",
    "df.set_index('date_time', inplace=True)\n",
    "\n",
    "print('filling empty title columns')\n",
    "#fills missing title with no_title\n",
    "df['title'].fillna('no_title', inplace = True)\n",
    "\n",
    "print('tokenizing titles')\n",
    "#tokenzies title\n",
    "df['title_tokens'] = df['title'].apply(lambda x: tokenizer(str(x), nlp))\n",
    "\n",
    "print('joining text and title')\n",
    "#joins title and text\n",
    "df['title_text'] = df['title_tokens'] + df['text_tokens']\n",
    "\n",
    "#display(df.head())\n",
    "display(f'df: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize polarity, avg tone\n",
    "#set new x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#setting x y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "#create vectorizer\n",
    "bagofwords = TfidfVectorizer(min_df=5)\n",
    "print('vectorizer done')\n",
    "\n",
    "#fit vectorizer\n",
    "print('beginng vectorizer fitting')\n",
    "bagofwords.fit(X_train)\n",
    "print('vectorizer fitting complete')\n",
    "\n",
    "\n",
    "#transform X_train\n",
    "print('beginning transformation')\n",
    "X_train_transformed = bagofwords.transform(X_train)\n",
    "print('X_train transformed')\n",
    "\n",
    "#transform X_test\n",
    "X_test_transformed = bagofwords.transform(X_test)\n",
    "print('X_test_transformed')\n",
    "\n",
    "#create model\n",
    "print('creating model')\n",
    "model = LogisticRegression(C=.1, solver='liblinear')\n",
    "print('model completed')\n",
    "\n",
    "\n",
    "#fit model\n",
    "print('fitting model')\n",
    "model.fit(X_train_transformed, y_train)\n",
    "print('model fitted')\n",
    "\n",
    "#score training set \n",
    "print('scoring training data')\n",
    "train_score = model.score(X_train_transformed, y_train)\n",
    "\n",
    "#score test set\n",
    "print('scoring test data')\n",
    "test_score = model.score(X_test_transformed, y_test)\n",
    "\n",
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Reg, GTrends Target, Numeric Tonal Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set X_Y\n",
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=.2, stratify=y)\n",
    "print(f'Split done - X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "estimators = [('model', LogisticRegression())]\n",
    "pipe = Pipeline(estimators, memory = cachedir)\n",
    "\n",
    "param_grid = [{'model': [LogisticRegression()],\n",
    " \n",
    "             'model__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "             'model__solver': ['liblinear', 'newton-cg', 'sag', 'saga','lbfgs']},\\\n",
    "              \n",
    "             {'model': [SVC()],\n",
    "            'model__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'model__C': [0.001, 0.01, 0.1, 1, 10]},\\\n",
    "              \n",
    "             {'model': [RandomForestClassifier()]},\n",
    "              \n",
    "             {'model': [SGDClassifier()],\n",
    "            'model__alpha': (0.00001, 0.000001),\n",
    "            'model__penalty': ('l2', 'elasticnet'),\n",
    "            'model__max_iter': (10, 50, 80)},\n",
    "              \n",
    "            {'model': [XGBClassifier(n_jobs=-1)],\n",
    "              'model__n_estimators': np.arange(1,500,100),\n",
    "             'model__learning_rate': [0.25, 0.5, 1]}]\n",
    "                       \n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "fittedgrid = grid.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedgrid.score(X_train_transformed, y_train)\n",
    "\n",
    "fittedgrid.score(X_test_transformed, y_test)\n",
    "\n",
    "fittedgrid.best_estimator_\n",
    "\n",
    "fittedgrid.best_params_\n",
    "\n",
    "fittedgrid.best_score_\n",
    "\n",
    "fittedgrid.param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Casuality and Time Series - TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
